{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6eebef-d04d-4b06-a783-c7f87a6b538a",
   "metadata": {},
   "source": [
    "ðŸ“Œ Project Introduction\n",
    "\n",
    "In this project, we use three different models to classify images of five different venues: decision tree supervised, decision tree semi-supervised and CNN.\n",
    "This notebook adress only the first model.\n",
    "\n",
    "ðŸ“Œ Supervised Learning\n",
    "Supervised learning uses only labeled data to train the model. By leveraging the structure of decision trees, this approach accurately connect input features to their corresponding labels for making the predictive performance better.\n",
    "\n",
    "ðŸ“Œ In this Project, we will:\n",
    "\n",
    "    Analyze all the classes in the data set (EDA= exploratory data analysis) \n",
    "    Build two models that can classify images into 5 venus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96186a48-6bdb-4e92-b128-b3cbebbca988",
   "metadata": {},
   "source": [
    "ðŸ“Œ Install  and importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2f079-9473-452c-ace9-1709adfdcd81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d455d2-f4cb-4750-8429-bd0bcf973ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip  install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3715d-1f85-4c28-83bb-0c2f9f84f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dec41-af30-4afe-a360-308770d87514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imagehash Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81136b44-39bb-4462-8bf1-5263824a52f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a1e58-a2ba-4267-a751-eaf3eea690fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip  install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce0829-30c4-4baa-9687-a19a99a207ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6454004-a3b0-4a2e-8aa2-f9d2a856207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from PIL import Image\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396cdc9e-7ec0-4b9a-8c67-72280fa4f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b93ad-9b7e-4a6e-8cf0-cdfbc13526bf",
   "metadata": {},
   "source": [
    "## Decision trees for supervised learning  -  original dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0a0b2-63e2-4219-afaa-05e214e832e1",
   "metadata": {},
   "source": [
    "#### Code for use gpu in Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e27b18-016d-4595-86da-cad6d5bf45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd42ec70-f97c-486e-b2ea-b7f25aba9a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-10.16-x86_64-i386-64bit\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e34440-bc71-4b6c-b8f9-c903a34537ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'final_dataset'\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "label_map = {\n",
    "    'bar': 0,\n",
    "    'beach': 1,\n",
    "    'bookstore': 2,\n",
    "    'restaurant': 3,\n",
    "    'subway': 4\n",
    "}\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ca314a-7290-4524-aa8d-047a6772fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = {\n",
    "'Classbar': len(os.listdir('final_dataset/bar')),\n",
    "'Classbeach': len(os.listdir('final_dataset/beach')),\n",
    "'Classbookstore': len(os.listdir('final_dataset/bookstore')),\n",
    "'Classrest': len(os.listdir('final_dataset/restaurant')),\n",
    "'Classsub': len(os.listdir('final_dataset/subway'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ad280-614a-4425-a856-ac90ed3e38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(number_classes.keys(), number_classes.values(), width = .5);\n",
    "plt.title(\"Number of Images by Class\");\n",
    "plt.xlabel('Class Name');\n",
    "plt.ylabel('# Images');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72afaa0a-e98e-4340-ac68-517bca6ee570",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b495e9f-b15d-4e1b-a030-f2a672e8757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish to read all bar images\n",
      "finish to read all beach images\n",
      "finish to read all bookstore images\n",
      "finish to read all restaurant images\n",
      "finish to read all subway images\n"
     ]
    }
   ],
   "source": [
    "for name, num in label_map.items():\n",
    "    for img_name in os.listdir(os.path.join(img_path, name)):\n",
    "        img_file = Image.open(os.path.join(img_path, name, img_name)).convert('RGB')\n",
    "        img = transforms(img_file).to(device)\n",
    "        images.append(img.cpu().numpy())\n",
    "        labels.append(num)\n",
    "    print(f\"finish to read all {name} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4a1a5-aa9f-4e99-90ee-199ee15df96a",
   "metadata": {},
   "source": [
    "ðŸ“Œ Data Splitting into images and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744ff47e-0f77-4721-b508-068d21c9c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664cff4-fc57-48f9-b4eb-a2f3a17d36a6",
   "metadata": {},
   "source": [
    "ðŸ“Œ Image Preprocessing:\n",
    "\n",
    "    Resizing: Given the variability in image dimensions, we choosee to rezise our dataset dimension to 32*32\n",
    "    Normalization: Normalize pixel values for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d483ca-8686-4927-b9a8-1e7d5e4bf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape\n",
    "X_reshape = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584968e2-530f-4ce3-9f6e-05c36df9d09d",
   "metadata": {},
   "source": [
    "ðŸ“Œ Data splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fdcde1-31df-400f-bb04-83befcb4e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_reshape,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064e2d9-a2fa-45dc-95e6-e8cc3565f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=35, min_samples_split=20, min_samples_leaf=5)\n",
    "decision_tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab5364-95f9-45c2-8259-92efc9660188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average='macro')\n",
    "recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "f1 = f1_score(Y_test, Y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}; Precision: {precision}; Recall: {recall}; F1: {f1}\")\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=label_map.keys())\n",
    "confusion_matrix_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675972a8-73a8-4f64-8c56-4a2d7993a0c5",
   "metadata": {},
   "source": [
    "## Decision trees for supervised learning  -  Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343e579-d436-4ba9-9ec3-74fd93e8c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'final_dataset'\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "label_map = {\n",
    "    'bar': 0,\n",
    "    'beach': 1,\n",
    "    'bookstore': 2,\n",
    "    'restaurant_augmented': 3,\n",
    "    'subway': 4\n",
    "}\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68989286-010a-4680-bea5-aadbcb659e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = {\n",
    "'Classbar': len(os.listdir('final_dataset/bar')),\n",
    "'Classbeach': len(os.listdir('final_dataset/beach')),\n",
    "'Classbookstore': len(os.listdir('final_dataset/bookstore')),\n",
    "'Classrest': len(os.listdir('final_dataset/restaurant_augmented')),\n",
    "'Classsub': len(os.listdir('final_dataset/subway'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b7594-e295-46fd-a314-cf0d090e5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(number_classes.keys(), number_classes.values(), width = .5);\n",
    "plt.title(\"Number of Images by Class\");\n",
    "plt.xlabel('Class Name');\n",
    "plt.ylabel('# Images');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbd2a9-11a2-4cdb-849f-239ca875fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1c134-38a3-467a-94ac-69e773b1602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, num in label_map.items():\n",
    "    for img_name in os.listdir(os.path.join(img_path, name)):\n",
    "        img_file = Image.open(os.path.join(img_path, name, img_name)).convert('RGB')\n",
    "        img = transforms(img_file).to(device)\n",
    "        images.append(img.cpu().numpy())\n",
    "        labels.append(num)\n",
    "    print(f\"finish to read all {name} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec00e8-f4d9-4040-83da-a61a9fd32fa3",
   "metadata": {},
   "source": [
    "ðŸ“Œ Data Splitting into images and labels on augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa23c77-cdbf-4998-ad94-da17c1d7b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c574d6-b244-4050-9b12-2da10e8fb09c",
   "metadata": {},
   "source": [
    "ðŸ“Œ Image Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be666c-3bf9-4438-abed-6066407b007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape\n",
    "X_reshape = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46ff68-c653-4566-bffd-83751a25eea4",
   "metadata": {},
   "source": [
    "ðŸ“Œ augmented Data splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d91e5-2350-4e6b-859f-85c292aeb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_reshape,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea543c3e-7dff-4567-8923-896969e72868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=35, min_samples_split=20, min_samples_leaf=5)\n",
    "decision_tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561222a-fd40-4893-8ba8-48388f1693ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average='macro')\n",
    "recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "f1 = f1_score(Y_test, Y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}; Precision: {precision}; Recall: {recall}; F1: {f1}\")\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=label_map.keys())\n",
    "confusion_matrix_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c7de7-49e6-4f18-8be9-7516f12e17aa",
   "metadata": {},
   "source": [
    "## Decision trees for supervised learning  -  augmented dataset with hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910e771b-432b-4a25-846a-f6f0f9ad71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d886b2-0868-46df-9da8-c443b9ea5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6a8f76-7966-4bc4-8df3-1958330ce676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56875eca-27bd-4d60-8312-a5c1c559394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': [ 'entropy'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, None],\n",
    "    'min_samples_split': [2, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print best parameters and scores\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "\n",
    "\n",
    "# Train model with best parameters\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "best_decision_tree.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "with open('best_decision_tree_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_decision_tree, model_file)\n",
    "\n",
    "# Evaluate the best model\n",
    "Y_pred = best_decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred, average='macro')\n",
    "recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "f1 = f1_score(Y_test, Y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}; Precision: {precision}; Recall: {recall}; F1: {f1}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=label_map.keys())\n",
    "confusion_matrix_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc2333-95b4-4c63-8ada-2eacc6a77ec0",
   "metadata": {},
   "source": [
    "After saving the best model, we try to test it on a new differents images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c715da-8aaf-4a5d-accb-a5d8fc814315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def classify_image(image_path, model_path='best_decision_tree_model.pkl'):\n",
    "\n",
    "\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Resize images to 32x32 pixels\n",
    "        transforms.ToTensor()         # Convert images to PyTorch tensors\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img_file = Image.open(image_path).convert('RGB')\n",
    "    img = transform(img_file).numpy()\n",
    "    img = img / 255.0\n",
    "    img = img.reshape(1, -1)  # Flatten the image\n",
    "\n",
    "\n",
    "    prediction = decision_tree.predict(img)\n",
    "\n",
    "    label_map = {\n",
    "        0: 'bar',\n",
    "        1: 'beach',\n",
    "        2: 'bookstore',\n",
    "        3: 'restaurant',\n",
    "        4: 'subway'\n",
    "    }\n",
    "\n",
    "    predicted_class = label_map[prediction[0]]\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "image_path = 'beach.jpeg'  \n",
    "predicted_class = classify_image(image_path)\n",
    "print(f'The image is classified as: {predicted_class}')\n",
    "\n",
    "\n",
    "\n",
    "image_path = 'beach.jpeg' \n",
    "predicted_class = classify_image(image_path)\n",
    "print(f'The image is classified as: {predicted_class}')\n",
    "\n",
    "\n",
    "image_path = 'beach.jpeg'  \n",
    "predicted_class = classify_image(image_path)\n",
    "print(f'The image is classified as: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df3098-c80f-4d8a-a625-93f3fd5593f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
